{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf84afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2 as cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d991ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24b0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"train.zip\",\"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097c29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train/asl_alphabet_train/asl_alphabet_train'\n",
    "test_dir = 'test/asl-alphabet-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b07ebecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_HEIGHT = 200\n",
    "STANDARD_WIDTH = 200\n",
    "MIN_CONFIDENCE_LEVEL = 0.7\n",
    "\n",
    "class MediaPipe(object):\n",
    "    def __call__(self, sample):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        image = np.array(sample)\n",
    "        mp_hands = mp.solutions.hands\n",
    "        with mp_hands.Hands(static_image_mode = True,max_num_hands = 2,\n",
    "            min_detection_confidence = MIN_CONFIDENCE_LEVEL) as hands:\n",
    "\n",
    "            #For training change this line, don't need to flip (since images appear to be from back-facing camera) \n",
    "            #Convert cv2 BGR image to RGB image and flip (since image coming from front-facing camera)  \n",
    "            processed = hands.process(cv2.flip(image, 1))\n",
    "\n",
    "            #No hand detected (Figure out how we want to handle, 126 vector with all 0s?): \n",
    "            if not processed.multi_hand_landmarks: \n",
    "                zeros = torch.tensor(np.array([0] * 126), dtype=torch.float32)\n",
    "                return zeros\n",
    "\n",
    "            feature_vector = [] \n",
    "            #Could have one or two hands: \n",
    "            for hand in processed.multi_hand_landmarks: \n",
    "                for curr_landmark in hand.landmark: \n",
    "                    x = curr_landmark.x \n",
    "                    feature_vector.append(x)\n",
    "\n",
    "                    y = curr_landmark.y \n",
    "                    feature_vector.append(y)\n",
    "\n",
    "                    z = curr_landmark.z\n",
    "                    feature_vector.append(z)\n",
    "\n",
    "            #If we have just one hand, zero out the remaining (to ensure constant vector size of 126)\n",
    "            #Might cause problems in one-hand case if we care which hand is visible/showing sign language\n",
    "            #Solution to this is to use processed.multi_handedness\n",
    "            if (len(feature_vector) == 63):\n",
    "                zero_vector = [0] * 63 \n",
    "                feature_vector.extend(zero_vector)\n",
    "            \n",
    "            output = torch.tensor(np.array(feature_vector), dtype=torch.float32)\n",
    "\n",
    "            return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0dcba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    MediaPipe(),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((200,200)),\n",
    "    MediaPipe(),\n",
    "])\n",
    "dataset = ImageFolder(train_dir, transform=transform)\n",
    "test = ImageFolder(test_dir, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd23920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.7970e-01,  7.7219e-01, -1.7897e-06,  1.5134e-01,  6.7943e-01,\n",
       "         -2.7918e-02,  7.9879e-02,  5.2470e-01, -4.5893e-02,  7.8037e-02,\n",
       "          3.9787e-01, -6.8957e-02,  1.0990e-01,  2.9550e-01, -8.4862e-02,\n",
       "          1.9695e-01,  4.0148e-01,  1.1828e-03,  1.6516e-01,  3.2843e-01,\n",
       "         -6.9060e-02,  1.5181e-01,  4.3508e-01, -1.1256e-01,  1.5143e-01,\n",
       "          5.3223e-01, -1.2950e-01,  2.8268e-01,  4.1856e-01, -1.3367e-02,\n",
       "          2.3814e-01,  3.6513e-01, -8.9069e-02,  2.2481e-01,  4.9813e-01,\n",
       "         -1.1013e-01,  2.2211e-01,  6.0418e-01, -1.0397e-01,  3.6469e-01,\n",
       "          4.5183e-01, -3.8334e-02,  3.0737e-01,  4.1583e-01, -1.1127e-01,\n",
       "          2.8970e-01,  5.4457e-01, -9.5069e-02,  2.8387e-01,  6.4001e-01,\n",
       "         -6.1881e-02,  4.4483e-01,  4.9975e-01, -6.5776e-02,  3.8048e-01,\n",
       "          4.7142e-01, -1.1261e-01,  3.5463e-01,  5.6246e-01, -9.3425e-02,\n",
       "          3.4747e-01,  6.2949e-01, -6.3755e-02,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00]),\n",
       " 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74cbeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = len(dataset)\n",
    "train_len_proportion = 0.9\n",
    "train_len = int(train_len_proportion * dataset_len)\n",
    "val_len = dataset_len - train_len\n",
    "train_dataset, val_dataset = random_split(dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c8a254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78300, 8700)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3aea981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size*2, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b2d551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "classes = dataset.classes\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e03a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "652fc9d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29479/1735971818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%5s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# iter_data = iter(train_dl)\n",
    "# fig, axes = plt.subplots(figsize=(12, 12), ncols=5)\n",
    "# for i in range(5):\n",
    "#     img, label = next(iter_data)\n",
    "#     ax = axes[i]\n",
    "#     ax.imshow(img[0].permute(1, 2, 0))\n",
    "#     ax.title.set_text(''.join('%5s' % classes[label[0]]))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf4e3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39bd55ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35e3407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(val_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "866f9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c27a5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66440880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLResnet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(126, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 29),\n",
    "        )\n",
    "        \n",
    "        self.network = self.linear_relu_stack\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "    \n",
    "    def freeze(self):\n",
    "        # To freeze the residual layers\n",
    "        for param in self.network.parameters():\n",
    "            param.require_grad = False\n",
    "#         for param in self.network.fc.parameters():\n",
    "#             param.require_grad = True\n",
    "    \n",
    "    def unfreeze(self):\n",
    "        # Unfreeze all layers\n",
    "        for param in self.network.parameters():\n",
    "            param.require_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2748a3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASLResnet(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=126, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=29, bias=True)\n",
       "  )\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=126, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=29, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = to_device(ASLResnet(), device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afa3575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in tqdm(val_loader)]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ab905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15d6aff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [07:57<00:00,  5.48s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 3.36788272857666, 'val_acc': 0.03206896409392357}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [evaluate(model, valid_dl)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1be2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8637ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e112b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze()\n",
    "epochs = 3\n",
    "max_lr = 1e-4\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f0947ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1566/1566 [1:04:09<00:00,  2.46s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [07:14<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 2.0145, val_acc: 0.4929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1566/1566 [1:04:03<00:00,  2.45s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [07:12<00:00,  4.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 1.6586, val_acc: 0.5867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1566/1566 [1:17:35<00:00,  2.97s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 87/87 [08:47<00:00,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], val_loss: 1.6139, val_acc: 0.6001\n",
      "CPU times: user 37.6 s, sys: 8.25 s, total: 45.9 s\n",
      "Wall time: 3h 49min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6a736d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'asl-colored-mediapipe-mvp2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac2ac7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 18/18 [00:36<00:00,  2.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 1.2204302549362183, 'val_acc': 0.6600000262260437}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test\n",
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "    return dataset.classes[preds[0].item()]\n",
    "len(test)\n",
    "test_dl = DataLoader(test, batch_size, num_workers=4, pin_memory=True)\n",
    "test_dl = DeviceDataLoader(test_dl, device)\n",
    "evaluate(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "302d4186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                    | 0/87 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▌                                           | 1/87 [00:00<00:59,  1.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|█                                           | 2/87 [00:01<00:41,  2.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|█▌                                          | 3/87 [00:01<00:35,  2.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|██                                          | 4/87 [00:01<00:32,  2.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|██▌                                         | 5/87 [00:02<00:31,  2.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|███                                         | 6/87 [00:02<00:29,  2.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|███▌                                        | 7/87 [00:02<00:29,  2.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|████                                        | 8/87 [00:03<00:28,  2.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|████▌                                       | 9/87 [00:03<00:27,  2.79it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|████▉                                      | 10/87 [00:03<00:27,  2.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█████▍                                     | 11/87 [00:04<00:26,  2.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█████▉                                     | 12/87 [00:04<00:26,  2.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|██████▍                                    | 13/87 [00:04<00:26,  2.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|██████▉                                    | 14/87 [00:05<00:25,  2.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|███████▍                                   | 15/87 [00:05<00:25,  2.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|███████▉                                   | 16/87 [00:05<00:25,  2.81it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|████████▍                                  | 17/87 [00:06<00:24,  2.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|████████▉                                  | 18/87 [00:06<00:24,  2.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|█████████▍                                 | 19/87 [00:07<00:24,  2.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|█████████▉                                 | 20/87 [00:07<00:23,  2.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██████████▍                                | 21/87 [00:07<00:23,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██████████▊                                | 22/87 [00:08<00:22,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|███████████▎                               | 23/87 [00:08<00:22,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|███████████▊                               | 24/87 [00:08<00:22,  2.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|████████████▎                              | 25/87 [00:09<00:21,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|████████████▊                              | 26/87 [00:09<00:21,  2.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|█████████████▎                             | 27/87 [00:09<00:21,  2.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|█████████████▊                             | 28/87 [00:10<00:20,  2.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|██████████████▎                            | 29/87 [00:10<00:20,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|██████████████▊                            | 30/87 [00:10<00:20,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███████████████▎                           | 31/87 [00:11<00:19,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███████████████▊                           | 32/87 [00:11<00:19,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|████████████████▎                          | 33/87 [00:11<00:18,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|████████████████▊                          | 34/87 [00:12<00:18,  2.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|█████████████████▎                         | 35/87 [00:12<00:18,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|█████████████████▊                         | 36/87 [00:13<00:17,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|██████████████████▎                        | 37/87 [00:13<00:17,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|██████████████████▊                        | 38/87 [00:13<00:17,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|███████████████████▎                       | 39/87 [00:14<00:16,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|███████████████████▊                       | 40/87 [00:14<00:16,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████████████████████▎                      | 41/87 [00:14<00:16,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████████████████████▊                      | 42/87 [00:15<00:15,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|█████████████████████▎                     | 43/87 [00:15<00:15,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████████████████████▋                     | 44/87 [00:15<00:15,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|██████████████████████▏                    | 45/87 [00:16<00:14,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|██████████████████████▋                    | 46/87 [00:16<00:14,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|███████████████████████▏                   | 47/87 [00:16<00:14,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|███████████████████████▋                   | 48/87 [00:17<00:13,  2.83it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|████████████████████████▏                  | 49/87 [00:17<00:13,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|████████████████████████▋                  | 50/87 [00:17<00:13,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████████████████████████▏                 | 51/87 [00:18<00:12,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████████████████████████▋                 | 52/87 [00:18<00:12,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████████████████████████▏                | 53/87 [00:18<00:11,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████████████████████████▋                | 54/87 [00:19<00:11,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|███████████████████████████▏               | 55/87 [00:19<00:11,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|███████████████████████████▋               | 56/87 [00:20<00:10,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|████████████████████████████▏              | 57/87 [00:20<00:10,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|████████████████████████████▋              | 58/87 [00:20<00:10,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|█████████████████████████████▏             | 59/87 [00:21<00:09,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|█████████████████████████████▋             | 60/87 [00:21<00:09,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████████████████████████████▏            | 61/87 [00:21<00:09,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|██████████████████████████████▋            | 62/87 [00:22<00:08,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████████████████████████████▏           | 63/87 [00:22<00:08,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████████████████████████████▋           | 64/87 [00:22<00:08,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|████████████████████████████████▏          | 65/87 [00:23<00:07,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|████████████████████████████████▌          | 66/87 [00:23<00:07,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|█████████████████████████████████          | 67/87 [00:23<00:06,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|█████████████████████████████████▌         | 68/87 [00:24<00:06,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|██████████████████████████████████         | 69/87 [00:24<00:06,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|██████████████████████████████████▌        | 70/87 [00:24<00:05,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|███████████████████████████████████        | 71/87 [00:25<00:05,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|███████████████████████████████████▌       | 72/87 [00:25<00:05,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████████████████████████████████       | 73/87 [00:26<00:04,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████████████████████████████████▌      | 74/87 [00:26<00:04,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|█████████████████████████████████████      | 75/87 [00:26<00:04,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|█████████████████████████████████████▌     | 76/87 [00:27<00:03,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|██████████████████████████████████████     | 77/87 [00:27<00:03,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|██████████████████████████████████████▌    | 78/87 [00:27<00:03,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|███████████████████████████████████████    | 79/87 [00:28<00:02,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|███████████████████████████████████████▌   | 80/87 [00:28<00:02,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|████████████████████████████████████████   | 81/87 [00:28<00:02,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|████████████████████████████████████████▌  | 82/87 [00:29<00:01,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████████████████████████████████████  | 83/87 [00:29<00:01,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████████████████████████████████████▌ | 84/87 [00:29<00:01,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|██████████████████████████████████████████ | 85/87 [00:30<00:00,  2.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|██████████████████████████████████████████▌| 86/87 [00:30<00:00,  2.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|███████████████████████████████████████████| 87/87 [00:30<00:00,  2.81it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.001335574546828866, 'val_acc': 1.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "model2 = to_device(ASLResnet(), device)\n",
    "model2.load_state_dict(torch.load('checkpoints/asl-colored-mediapipe-mvp.pth'))\n",
    "evaluate(model2, valid_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228661cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
