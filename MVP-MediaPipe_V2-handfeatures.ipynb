{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf84afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2 as cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9d991ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d24b0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"train.zip\",\"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "097c29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train/asl_alphabet_train/asl_alphabet_train'\n",
    "test_dir = 'test/asl-alphabet-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b07ebecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_HEIGHT = 200\n",
    "STANDARD_WIDTH = 200\n",
    "MIN_CONFIDENCE_LEVEL = 0.7\n",
    "\n",
    "class MediaPipe(object):\n",
    "    def __call__(self, sample):\n",
    "        image = np.array(sample)\n",
    "        mp_hands = mp.solutions.hands\n",
    "        with mp_hands.Hands(static_image_mode = True,max_num_hands = 2,\n",
    "            min_detection_confidence = MIN_CONFIDENCE_LEVEL) as hands:\n",
    "\n",
    "            #For training change this line, don't need to flip (since images appear to be from back-facing camera) \n",
    "            #Convert cv2 BGR image to RGB image and flip (since image coming from front-facing camera)  \n",
    "            processed = hands.process(cv2.flip(image, 1))\n",
    "\n",
    "            #No hand detected (Figure out how we want to handle, 126 vector with all 0s?): \n",
    "            if not processed.multi_hand_landmarks: \n",
    "                zeros = torch.tensor(np.array([0] * 126), dtype=torch.float32)\n",
    "                return zeros\n",
    "\n",
    "            feature_vector = [] \n",
    "            #Could have one or two hands: \n",
    "            for hand in processed.multi_hand_landmarks: \n",
    "                for curr_landmark in hand.landmark: \n",
    "                    x = curr_landmark.x \n",
    "                    feature_vector.append(x)\n",
    "\n",
    "                    y = curr_landmark.y \n",
    "                    feature_vector.append(y)\n",
    "\n",
    "                    z = curr_landmark.z\n",
    "                    feature_vector.append(z)\n",
    "\n",
    "            #If we have just one hand, zero out the remaining (to ensure constant vector size of 126)\n",
    "            #Might cause problems in one-hand case if we care which hand is visible/showing sign language\n",
    "            #Solution to this is to use processed.multi_handedness\n",
    "            if (len(feature_vector) == 63):\n",
    "                zero_vector = [0] * 63 \n",
    "                feature_vector.extend(zero_vector)\n",
    "            \n",
    "            output = torch.tensor(np.array(feature_vector), dtype=torch.float32)\n",
    "\n",
    "            return output\n",
    "\n",
    "class ExtractHandFeatures: \n",
    "    def __call__(self, sample):\n",
    "        image = np.array(sample)\n",
    "        mp_hands = mp.solutions.hands\n",
    "        with mp_hands.Hands(static_image_mode = True,max_num_hands = 2,\n",
    "            min_detection_confidence = MIN_CONFIDENCE_LEVEL) as hands:\n",
    "            \n",
    "            #For training change this line, don't need to flip (since images appear to be from back-facing camera) \n",
    "            #Convert cv2 BGR image to RGB image and flip (since image coming from front-facing camera)  \n",
    "            processed = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))\n",
    "            # processed = hands.process(cv2.cvtColor(self.raw_image, cv2.COLOR_BGR2RGB)) \n",
    "\n",
    "            #No hand detected (Figure out how we want to handle, 126 vector with all 0s?): \n",
    "            if not processed.multi_hand_landmarks: \n",
    "                zeros = torch.tensor(np.array([0] * 126), dtype=torch.float32)\n",
    "                return zeros\n",
    "            \n",
    "            feature_vector = []         \n",
    "            hands = [] \n",
    "\n",
    "            for idx, hand_handedness in enumerate(processed.multi_handedness):\n",
    "                hands.append(hand_handedness.classification[0].label)\n",
    "                \n",
    "\n",
    "            #Left hand is first 63, Right hand is last 63\n",
    "            #LEFT HAND ONLY CASE: \n",
    "            if (len(hands) == 1 and hands[0] == \"Left\"):\n",
    "                for hand in processed.multi_hand_landmarks: \n",
    "                    for curr_landmark in hand.landmark: \n",
    "                        x = curr_landmark.x \n",
    "                        feature_vector.append(x)\n",
    "\n",
    "                        y = curr_landmark.y \n",
    "                        feature_vector.append(y)\n",
    "\n",
    "                        z = curr_landmark.z\n",
    "                        feature_vector.append(z)\n",
    "                zero_vector = [0] * 63 \n",
    "                feature_vector.extend(zero_vector)\n",
    "\n",
    "            #RIGHT HAND ONLY CASE: \n",
    "            if (len(hands) == 1 and hands[0] == \"Right\"):\n",
    "                # print(\"Detected only right hand\")\n",
    "                for hand in processed.multi_hand_landmarks: \n",
    "                    for curr_landmark in hand.landmark: \n",
    "                        x = curr_landmark.x \n",
    "                        feature_vector.append(x)\n",
    "\n",
    "                        y = curr_landmark.y \n",
    "                        feature_vector.append(y)\n",
    "\n",
    "                        z = curr_landmark.z\n",
    "                        feature_vector.append(z)\n",
    "                zero_vector = [0] * 63 \n",
    "                feature_vector = zero_vector + feature_vector\n",
    "            \n",
    "            #BOTH HANDS CASE: \n",
    "            if (len(hands) == 2):\n",
    "                # print(\"Detected both hands\")\n",
    "                zeros = torch.tensor(np.array([0] * 126), dtype=torch.float32)\n",
    "                return zeros\n",
    "\n",
    "            output = torch.tensor(np.array(feature_vector), dtype=torch.float32)\n",
    "            #print(output)\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0dcba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    ExtractHandFeatures(),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((200,200)),\n",
    "    ExtractHandFeatures(),\n",
    "])\n",
    "dataset = ImageFolder(train_dir, transform=transform)\n",
    "test = ImageFolder(test_dir, transform=test_transform)\n",
    "# dataset = DatasetFolder(train_dir)\n",
    "# test = DatasetFolder(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cd23920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0.]),\n",
       " 0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74cbeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = len(dataset)\n",
    "train_len_proportion = 0.9\n",
    "train_len = int(train_len_proportion * dataset_len)\n",
    "val_len = dataset_len - train_len\n",
    "train_dataset, val_dataset = random_split(dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8c8a254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78300, 8700)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3aea981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size*2, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2b2d551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "classes = dataset.classes\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "652fc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_data = iter(train_dl)\n",
    "# fig, axes = plt.subplots(figsize=(12, 12), ncols=5)\n",
    "# for i in range(5):\n",
    "#     img, label = next(iter_data)\n",
    "#     ax = axes[i]\n",
    "#     ax.imshow(img[0].permute(1, 2, 0))\n",
    "#     ax.title.set_text(''.join('%5s' % classes[label[0]]))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf4e3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39bd55ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35e3407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(val_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "866f9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c27a5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66440880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLResnet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(126, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(128, 29),            \n",
    "        )\n",
    "        \n",
    "        self.network = self.linear_relu_stack\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "    \n",
    "    def freeze(self):\n",
    "        # To freeze the residual layers\n",
    "        for param in self.network.parameters():\n",
    "            param.require_grad = False\n",
    "#         for param in self.network.fc.parameters():\n",
    "#             param.require_grad = True\n",
    "    \n",
    "    def unfreeze(self):\n",
    "        # Unfreeze all layers\n",
    "        for param in self.network.parameters():\n",
    "            param.require_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2748a3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASLResnet(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=126, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Dropout(p=0.25, inplace=False)\n",
       "    (13): Linear(in_features=128, out_features=29, bias=True)\n",
       "  )\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=126, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Dropout(p=0.25, inplace=False)\n",
       "    (13): Linear(in_features=128, out_features=29, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = to_device(ASLResnet(), device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afa3575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in tqdm(val_loader)]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ab905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15d6aff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 87/87 [09:59<00:00,  6.89s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 3.368922472000122, 'val_acc': 0.03379310294985771}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [evaluate(model, valid_dl)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1be2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8637ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e112b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.freeze()\n",
    "epochs = 5\n",
    "max_lr = 1e-4\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0947ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1566/1566 [1:29:47<00:00,  3.44s/it]\n",
      "100%|███████████████████████████████████████████| 87/87 [09:49<00:00,  6.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 3.3651, val_acc: 0.0990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1566/1566 [1:27:41<00:00,  3.36s/it]\n",
      "100%|███████████████████████████████████████████| 87/87 [09:57<00:00,  6.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 3.2079, val_acc: 0.1244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1566/1566 [1:28:20<00:00,  3.38s/it]\n",
      "100%|███████████████████████████████████████████| 87/87 [10:06<00:00,  6.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], val_loss: 3.0777, val_acc: 0.1330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1566/1566 [1:27:16<00:00,  3.34s/it]\n",
      "100%|███████████████████████████████████████████| 87/87 [09:54<00:00,  6.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], val_loss: 3.0248, val_acc: 0.1402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████▎       | 1239/1566 [1:09:19<16:46,  3.08s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a736d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'asl-colored-extracthand-mvp3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ac7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test\n",
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "    return dataset.classes[preds[0].item()]\n",
    "len(test)\n",
    "test_dl = DataLoader(test, batch_size, num_workers=4, pin_memory=True)\n",
    "test_dl = DeviceDataLoader(test_dl, device)\n",
    "evaluate(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "model2 = to_device(ASLResnet(), device)\n",
    "model2.load_state_dict(torch.load('asl-colored-extracthand-mvp3.pth'))\n",
    "evaluate(model2, valid_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228661cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
