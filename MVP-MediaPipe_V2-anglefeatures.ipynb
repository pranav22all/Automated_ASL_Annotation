{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf84afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import cv2 as cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d991ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24b0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"train.zip\",\"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097c29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train/asl_alphabet_train/asl_alphabet_train'\n",
    "test_dir = 'test/asl-alphabet-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b07ebecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARD_HEIGHT = 200\n",
    "STANDARD_WIDTH = 200\n",
    "MIN_CONFIDENCE_LEVEL = 0.7\n",
    "\n",
    "class MediaPipe(object):\n",
    "    def __call__(self, sample):\n",
    "        image = np.array(sample)\n",
    "        mp_hands = mp.solutions.hands\n",
    "        with mp_hands.Hands(static_image_mode = True,max_num_hands = 2,\n",
    "            min_detection_confidence = MIN_CONFIDENCE_LEVEL) as hands:\n",
    "\n",
    "            #For training change this line, don't need to flip (since images appear to be from back-facing camera) \n",
    "            #Convert cv2 BGR image to RGB image and flip (since image coming from front-facing camera)  \n",
    "            processed = hands.process(cv2.flip(image, 1))\n",
    "\n",
    "            #No hand detected (Figure out how we want to handle, 126 vector with all 0s?): \n",
    "            if not processed.multi_hand_landmarks: \n",
    "                zeros = torch.tensor(np.array([0] * 126), dtype=torch.float32)\n",
    "                return zeros\n",
    "\n",
    "            feature_vector = [] \n",
    "            #Could have one or two hands: \n",
    "            for hand in processed.multi_hand_landmarks: \n",
    "                for curr_landmark in hand.landmark: \n",
    "                    x = curr_landmark.x \n",
    "                    feature_vector.append(x)\n",
    "\n",
    "                    y = curr_landmark.y \n",
    "                    feature_vector.append(y)\n",
    "\n",
    "                    z = curr_landmark.z\n",
    "                    feature_vector.append(z)\n",
    "\n",
    "            #If we have just one hand, zero out the remaining (to ensure constant vector size of 126)\n",
    "            #Might cause problems in one-hand case if we care which hand is visible/showing sign language\n",
    "            #Solution to this is to use processed.multi_handedness\n",
    "            if (len(feature_vector) == 63):\n",
    "                zero_vector = [0] * 63 \n",
    "                feature_vector.extend(zero_vector)\n",
    "            \n",
    "            output = torch.tensor(np.array(feature_vector), dtype=torch.float32)\n",
    "\n",
    "            return output\n",
    "\n",
    "class ExtractHandFeatures: \n",
    "    def __call__(self, sample):\n",
    "        image = np.array(sample)\n",
    "        mp_hands = mp.solutions.hands\n",
    "        with mp_hands.Hands(static_image_mode = True,max_num_hands = 2,\n",
    "            min_detection_confidence = MIN_CONFIDENCE_LEVEL) as hands:\n",
    "            \n",
    "            #For training change this line, don't need to flip (since images appear to be from back-facing camera) \n",
    "            #Convert cv2 BGR image to RGB image and flip (since image coming from front-facing camera)  \n",
    "            processed = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))\n",
    "            # processed = hands.process(cv2.cvtColor(self.raw_image, cv2.COLOR_BGR2RGB)) \n",
    "\n",
    "            #No hand detected (Figure out how we want to handle, 126 vector with all 0s?): \n",
    "            if not processed.multi_hand_landmarks: \n",
    "                zeros = torch.tensor(np.array([0] * 126), dtype=torch.float32)\n",
    "                return zeros\n",
    "            \n",
    "            feature_vector = []         \n",
    "            hands = [] \n",
    "\n",
    "            for idx, hand_handedness in enumerate(processed.multi_handedness):\n",
    "                hands.append(hand_handedness.classification[0].label)\n",
    "                \n",
    "\n",
    "            #Left hand is first 63, Right hand is last 63\n",
    "            #LEFT HAND ONLY CASE: \n",
    "            if (len(hands) == 1 and hands[0] == \"Left\"):\n",
    "                for hand in processed.multi_hand_landmarks: \n",
    "                    for curr_landmark in hand.landmark: \n",
    "                        x = curr_landmark.x \n",
    "                        feature_vector.append(x)\n",
    "\n",
    "                        y = curr_landmark.y \n",
    "                        feature_vector.append(y)\n",
    "\n",
    "                        z = curr_landmark.z\n",
    "                        feature_vector.append(z)\n",
    "                zero_vector = [0] * 63 \n",
    "                feature_vector.extend(zero_vector)\n",
    "\n",
    "            #RIGHT HAND ONLY CASE: \n",
    "            if (len(hands) == 1 and hands[0] == \"Right\"):\n",
    "#                 print(\"Detected only right hand\")\n",
    "                for hand in processed.multi_hand_landmarks: \n",
    "                    for curr_landmark in hand.landmark: \n",
    "                        x = curr_landmark.x \n",
    "                        feature_vector.append(x)\n",
    "\n",
    "                        y = curr_landmark.y \n",
    "                        feature_vector.append(y)\n",
    "\n",
    "                        z = curr_landmark.z\n",
    "                        feature_vector.append(z)\n",
    "                zero_vector = [0] * 63 \n",
    "                feature_vector = zero_vector + feature_vector\n",
    "            \n",
    "            #BOTH HANDS CASE: \n",
    "            if (len(hands) == 2):\n",
    "                # print(\"Detected both hands\")\n",
    "                zeros = torch.tensor(np.array([0] * 126), dtype=torch.float32)\n",
    "                return zeros\n",
    "\n",
    "            output = torch.tensor(np.array(feature_vector), dtype=torch.float32)\n",
    "            #print(output)\n",
    "            return output\n",
    "\n",
    "\n",
    "def get_angle_between_vectors(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    dot_product = np.dot(u, v)\n",
    "    norm = np.linalg.norm(u) * np.linalg.norm(v)\n",
    "    return np.arccos(dot_product / norm)        \n",
    "\n",
    "class ExtractAngleFeatures: \n",
    "    def __call__(self, sample):\n",
    "        image = np.array(sample)\n",
    "        mp_hands = mp.solutions.hands\n",
    "        with mp_hands.Hands(static_image_mode = True,max_num_hands = 2,\n",
    "            min_detection_confidence = MIN_CONFIDENCE_LEVEL) as hands:\n",
    "\n",
    "            #For training change this line, don't need to flip (since images appear to be from back-facing camera) \n",
    "            #Convert cv2 BGR image to RGB image and flip (since image coming from front-facing camera)  \n",
    "            processed = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))\n",
    "            #processed = hands.process(cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB)) #CHANGE THIS TO self.raw_image \n",
    "\n",
    "            #No hand detected (Figure out how we want to handle, 882 vector with all 0s?): \n",
    "            if not processed.multi_hand_landmarks: \n",
    "#                 print(\"no hand\")\n",
    "                zeros = torch.tensor(np.array([0] * 882), dtype=torch.float32)\n",
    "                return zeros\n",
    "\n",
    "            angles_list = []         \n",
    "            hands = [] \n",
    "\n",
    "            for idx, hand_handedness in enumerate(processed.multi_handedness):\n",
    "                hands.append(hand_handedness.classification[0].label)\n",
    "\n",
    "            #LEFT HAND ONLY CASE: \n",
    "            if (len(hands) == 1 and hands[0] == \"Left\"):\n",
    "\n",
    "                landmarks = np.zeros((21, 3))\n",
    "                index = 0 \n",
    "#                 print(\"Detected only left hand\")\n",
    "                for hand in processed.multi_hand_landmarks:   \n",
    "                    for curr_landmark in hand.landmark: \n",
    "                        x = curr_landmark.x \n",
    "                        y = curr_landmark.y \n",
    "                        z = curr_landmark.z\n",
    "                        landmarks[index] = [x, y, z]\n",
    "                        index += 1\n",
    "\n",
    "                # print(\"Landmarks is:\")\n",
    "                # print(landmarks)\n",
    "\n",
    "                connections = mp_hands.HAND_CONNECTIONS\n",
    "                # print(connections)\n",
    "                # print(len(connections))\n",
    "\n",
    "                difference_connect_vector = list(map(lambda t: landmarks[t[1]] - landmarks[t[0]], connections))\n",
    "                # print(difference_connect_vector)\n",
    "                # print(len(difference_connect_vector))\n",
    "\n",
    "                for connection_from in difference_connect_vector:\n",
    "                    for connection_to in difference_connect_vector:\n",
    "                        angle = get_angle_between_vectors(connection_from, connection_to)\n",
    "                        # If the angle is not null we store it else we store 0\n",
    "                        if angle == angle:\n",
    "                            angles_list.append(angle)\n",
    "                        else:\n",
    "                            angles_list.append(0)\n",
    "                # print(\"Angles list is:\")\n",
    "                # print(angles_list)\n",
    "                # print(len(angles_list))\n",
    "                zero_vector = [0] * 441 \n",
    "                angles_list.extend(zero_vector)\n",
    "\n",
    "            #RIGHT HAND ONLY CASE: \n",
    "            if (len(hands) == 1 and hands[0] == \"Right\"):\n",
    "                landmarks = np.zeros((21, 3))\n",
    "                index = 0 \n",
    "#                 print(\"Detected only right hand\")\n",
    "                for hand in processed.multi_hand_landmarks:   \n",
    "                    for curr_landmark in hand.landmark: \n",
    "                        x = curr_landmark.x \n",
    "                        y = curr_landmark.y \n",
    "                        z = curr_landmark.z\n",
    "                        landmarks[index] = [x, y, z]\n",
    "                        index += 1\n",
    "\n",
    "                # print(\"Landmarks is:\")\n",
    "                # print(landmarks)\n",
    "                connections = mp_hands.HAND_CONNECTIONS\n",
    "                # print(connections)\n",
    "                # print(len(connections))\n",
    "\n",
    "                difference_connect_vector = list(map(lambda t: landmarks[t[1]] - landmarks[t[0]], connections))\n",
    "                # print(difference_connect_vector)\n",
    "                # print(len(difference_connect_vector))\n",
    "\n",
    "                for connection_from in difference_connect_vector:\n",
    "                    for connection_to in difference_connect_vector:\n",
    "                        angle = get_angle_between_vectors(connection_from, connection_to)\n",
    "                        # If the angle is not null we store it else we store 0\n",
    "                        if angle == angle:\n",
    "                            angles_list.append(angle)\n",
    "                        else:\n",
    "                            angles_list.append(0)\n",
    "                zero_vector = [0] * 441 \n",
    "                angles_list = zero_vector + angles_list\n",
    "\n",
    "            #BOTH HANDS CASE: \n",
    "            if (len(hands) == 2):\n",
    "                print(\"Detected both hands\")\n",
    "                zeros = torch.tensor(np.array([0] * 882), dtype=torch.float32)\n",
    "                return zeros\n",
    "\n",
    "            output = torch.tensor(np.array(angles_list), dtype=torch.float32)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0dcba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    ExtractAngleFeatures(),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((200,200)),\n",
    "    ExtractAngleFeatures(),\n",
    "])\n",
    "dataset = ImageFolder(train_dir, transform=transform)\n",
    "test = ImageFolder(test_dir, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd23920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74cbeffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = len(dataset)\n",
    "train_len_proportion = 0.9\n",
    "train_len = int(train_len_proportion * dataset_len)\n",
    "val_len = dataset_len - train_len\n",
    "train_dataset, val_dataset = random_split(dataset, [train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8c8a254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78300, 8700)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3aea981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size*2, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b2d551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "classes = dataset.classes\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "652fc9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_data = iter(train_dl)\n",
    "# fig, axes = plt.subplots(figsize=(12, 12), ncols=5)\n",
    "# for i in range(5):\n",
    "#     img, label = next(iter_data)\n",
    "#     ax = axes[i]\n",
    "#     ax.imshow(img[0].permute(1, 2, 0))\n",
    "#     ax.title.set_text(''.join('%5s' % classes[label[0]]))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf4e3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39bd55ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35e3407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(val_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866f9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c27a5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66440880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLResnet(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(882, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(128, 29),            \n",
    "        )\n",
    "        \n",
    "        self.network = self.linear_relu_stack\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "    \n",
    "    def freeze(self):\n",
    "        # To freeze the residual layers\n",
    "        for param in self.network.parameters():\n",
    "            param.require_grad = False\n",
    "#         for param in self.network.fc.parameters():\n",
    "#             param.require_grad = True\n",
    "    \n",
    "    def unfreeze(self):\n",
    "        # Unfreeze all layers\n",
    "        for param in self.network.parameters():\n",
    "            param.require_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2748a3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASLResnet(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=882, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Dropout(p=0.25, inplace=False)\n",
       "    (13): Linear(in_features=128, out_features=29, bias=True)\n",
       "  )\n",
       "  (network): Sequential(\n",
       "    (0): Linear(in_features=882, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): ReLU()\n",
       "    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): Dropout(p=0.25, inplace=False)\n",
       "    (13): Linear(in_features=128, out_features=29, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = to_device(ASLResnet(), device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afa3575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in tqdm(val_loader)]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up custom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in tqdm(train_loader):\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15d6aff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/87 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "100%|███████████████████████████████████████████| 87/87 [10:38<00:00,  7.34s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 3.3680648803710938, 'val_acc': 0.03735632076859474}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [evaluate(model, valid_dl)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1be2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8637ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e112b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.freeze()\n",
    "epochs = 5\n",
    "max_lr = 1e-4\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0947ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1566 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      " 87%|████████████████████████████████▏    | 1362/1566 [1:19:25<11:36,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected both hands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████▉ | 1520/1566 [1:28:25<02:37,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected both hands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1566/1566 [1:31:02<00:00,  3.49s/it]\n",
      "  0%|                                                    | 0/87 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "100%|███████████████████████████████████████████| 87/87 [10:07<00:00,  6.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 3.7160, val_acc: 0.1040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1566 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "  7%|██▌                                   | 105/1566 [06:13<1:25:24,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected both hands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████▉            | 1053/1566 [1:00:26<29:17,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected both hands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1566/1566 [1:29:40<00:00,  3.44s/it]\n",
      "  0%|                                                    | 0/87 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "100%|███████████████████████████████████████████| 87/87 [10:09<00:00,  7.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 3.9499, val_acc: 0.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1566 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "  2%|▉                                      | 39/1566 [02:19<1:28:49,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected both hands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████▋                | 927/1566 [53:39<37:38,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected both hands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1566/1566 [1:30:26<00:00,  3.47s/it]\n",
      "  0%|                                                    | 0/87 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "100%|███████████████████████████████████████████| 87/87 [10:04<00:00,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], val_loss: 3.6111, val_acc: 0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1566 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      " 16%|█████▉                                | 243/1566 [14:03<1:16:57,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected both hands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████             | 1044/1566 [59:37<29:19,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected both hands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1566/1566 [1:29:00<00:00,  3.41s/it]\n",
      "  0%|                                                    | 0/87 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "100%|███████████████████████████████████████████| 87/87 [10:15<00:00,  7.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], val_loss: 3.5559, val_acc: 0.1325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1566 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:115: RuntimeWarning: invalid value encountered in arccos\n",
      " 26%|█████████▊                            | 402/1566 [23:03<1:06:15,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected both hands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████             | 1044/1566 [59:34<29:27,  3.39s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a736d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'asl-colored-extractangle-mvp3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ac7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test\n",
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "    return dataset.classes[preds[0].item()]\n",
    "len(test)\n",
    "test_dl = DataLoader(test, batch_size, num_workers=4, pin_memory=True)\n",
    "test_dl = DeviceDataLoader(test_dl, device)\n",
    "evaluate(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "model2 = to_device(ASLResnet(), device)\n",
    "model2.load_state_dict(torch.load('asl-colored-extractangle-mvp3.pth'))\n",
    "evaluate(model2, valid_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228661cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
